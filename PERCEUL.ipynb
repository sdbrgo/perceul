{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1e6G9G41BAeiZGj1GoxGu91097sPFsusg",
      "authorship_tag": "ABX9TyNnaaEQPWqAk4aH5GRnwFjd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdbrgo/PERCEUL/blob/umap-hdbscan/PERCEUL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "lIgbs-9bAhGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hdbscan\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "# pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# preprocesing\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin # used to define NumericSelector(), which is used in preprocessing\n",
        "\n",
        "# dimensionality reduction\n",
        "from sklearn.decomposition import PCA\n",
        "from umap import UMAP\n",
        "\n",
        "# cluster validation\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# clustering\n",
        "from sklearn.cluster import KMeans\n",
        "import hdbscan"
      ],
      "metadata": {
        "id": "Hmun0waZAq1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount GDrive"
      ],
      "metadata": {
        "id": "Azdyovt6i3GC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kAGfM795i2DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dataset"
      ],
      "metadata": {
        "id": "hIaAGxgMAsYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_name = \"\"\n",
        "df = pd.read_csv(ds_name)"
      ],
      "metadata": {
        "id": "rY1VJeKzAxNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "XTN8u1XHAxuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#=====================================================================\n",
        "# a custom and dynamic function for selecting numeric columns only.\n",
        "# will be used to make the pipeline\n",
        "class NumericSelector(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.numeric_cols_ = X.select_dtypes(include=[float, int]).columns\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_num = X[self.numeric_cols_]\n",
        "        return X_num\n",
        "#=====================================================================\n",
        "\n",
        "si = SimpleImputer(strategy='median')\n",
        "df_i = si.fit_transform(df)\n",
        "df_i = pd.DataFrame(df_i, columns=df.columns)\n",
        "\n",
        "ss = StandardScaler()\n",
        "df_i_ss = ss.fit_transform(df_i)\n",
        "df_p1 = pd.DataFrame(df_i_ss, columns=df.columns) # will undergo cluster exploration\n",
        "df_p2 = df_p1.copy() # will undergo final clustering"
      ],
      "metadata": {
        "id": "YbYB-pn6A5w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality Reduction 1"
      ],
      "metadata": {
        "id": "6xcgb0kjA-dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "umap_model = UMAP(n_components=2, random_state=42)\n",
        "df_p1 = umap_model.fit_transform(df_p1)"
      ],
      "metadata": {
        "id": "HdqWo-apt6To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Exploration\n"
      ],
      "metadata": {
        "id": "1sj9wcBpBDFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer1 = hdbscan.HDBSCAN(min_cluster_size=5, gen_min_span_tree=True)\n",
        "clusterer1.fit(df_p1)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.scatter(df_p1.iloc[:, 0], df_p1.iloc[:, 1], c=clusterer1.labels_, cmap='Paired')\n",
        "plt.title('HDBSCAN Clustering')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qSQdUZhBBFgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality Reduction 2"
      ],
      "metadata": {
        "id": "yrI7ZojQP8X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "df_p2 = pca.fit_transform(df_p2)"
      ],
      "metadata": {
        "id": "FGn6T4s7P_Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Clustering"
      ],
      "metadata": {
        "id": "hKbh6GTwBF8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================\n",
        "# function to identify the best `k` value.\n",
        "# put this inside `cluster_utils.py`\n",
        "def choose_k(X_pca, k_range=(2, 12)):\n",
        "    best_k = 2\n",
        "    best_score = -1\n",
        "\n",
        "    for k in range(k_range[0], k_range[1]):\n",
        "        km = KMeans(n_clusters=k, random_state=42)\n",
        "        labels = km.fit_predict(X_pca)\n",
        "        score = silhouette_score(X_pca, labels)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_k = k\n",
        "\n",
        "    return best_k\n",
        "#============================================================\n",
        "\n",
        "best_k = choose_k(df_p2) # pre-defined function\n",
        "\n",
        "km_final = Kmeans(n_clusters=best_k, random_state=42)\n",
        "labels = km_final.fit_predict(df_p2)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(df_p2[:, 0], df_p2[:, 1], c=labels, s=15)\n",
        "plt.title('K-Means Clustering')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "zTV8JaP8BP0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Analysis"
      ],
      "metadata": {
        "id": "iDj7TmV_BJO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================\n",
        "# place these functions in `cluster_utils.py` file\n",
        "def compute_cluster_stats(X_processed, labels, feature_names):\n",
        "    df = pd.DataFrame(X_processed, columns=feature_names)\n",
        "    df['cluster'] = labels\n",
        "\n",
        "    stats = {}\n",
        "\n",
        "    for cluster_id in sorted(df['cluster'].unique()):\n",
        "        cluster_data = df[df['cluster'] == cluster_id].drop(columns=['cluster'])\n",
        "\n",
        "        stats[cluster_id] = {\n",
        "            \"count\": len(cluster_data),\n",
        "            \"mean\": cluster_data.mean().to_dict(),\n",
        "            \"median\": cluster_data.median().to_dict(),\n",
        "            \"std\": cluster_data.std().to_dict(),\n",
        "            \"min\": cluster_data.min().to_dict(),\n",
        "            \"max\": cluster_data.max().to_dict(),\n",
        "            \"range\": (cluster_data.max() - cluster_data.min()).to_dict(),\n",
        "        }\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "def identify_extreme_features(X_processed, labels, feature_names, threshold=1.0):\n",
        "    df = pd.DataFrame(X_processed, columns=feature_names)\n",
        "    df['cluster'] = labels\n",
        "\n",
        "    global_mean = df.drop(columns=['cluster']).mean()\n",
        "    global_std = df.drop(columns=['cluster']).std()\n",
        "\n",
        "    extremes = {}\n",
        "\n",
        "    for cluster_id in sorted(df['cluster'].unique()):\n",
        "        cluster_mean = df[df['cluster'] == cluster_id].drop(columns=['cluster']).mean()\n",
        "\n",
        "        z_scores = ((cluster_mean - global_mean) / global_std).abs()\n",
        "\n",
        "        extreme_features = z_scores[z_scores > threshold].sort_values(ascending=False)\n",
        "\n",
        "        extremes[cluster_id] = {\n",
        "            \"features\": extreme_features.index.tolist(),\n",
        "            \"z_scores\": extreme_features.to_dict()\n",
        "        }\n",
        "\n",
        "    return extremes\n",
        "#============================================================\n",
        "\n",
        "cluster_stats = compute_cluster_stats(df_p2, labels, df_p2.columns)\n",
        "extreme_features = identify_extreme_features(df_p2, labels, df_p2.columns)\n",
        "\n",
        "# show cluster stats\n",
        "for cluster_id in cluster_stats.keys():\n",
        "    print(f\"Cluster {cluster_id} Summary\")\n",
        "\n",
        "    print(\"\\nExtreme Features:\")\n",
        "    print(json.dumps(extreme_features[cluster_id], indent=4))\n",
        "\n",
        "    print(\"\\nStatistics:\")\n",
        "    print(json.dumps(cluster_stats[cluster_id], indent=4))\n",
        "\n",
        "    print(\"\\n---\\n\")"
      ],
      "metadata": {
        "id": "2iW8ss3DBS46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the PERCEUL Pipeline"
      ],
      "metadata": {
        "id": "mGuCHjjZ6KvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline for Cluster Exploration\n",
        "exploration_pipeline = Pipeline(steps = [\n",
        "    ('numeric_selector', NumericSelector()),\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('umap_model', UMAP(n_components=2, random_state=42))\n",
        "])\n",
        "\n",
        "# pipeline for Final Clustering (Production)\n",
        "core_pipeline = Pipeline(steps = [\n",
        "    ('numeric_selector', NumericSelector()),\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=2))\n",
        "])"
      ],
      "metadata": {
        "id": "wEzImQse6OAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the Pipeline"
      ],
      "metadata": {
        "id": "hJ1BJBwbVlRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(exploration_pipeline, 'exploration_pipeline.pkl')\n",
        "joblib.dump(core_pipeline, 'core_pipeline.pkl')"
      ],
      "metadata": {
        "id": "Rq5RYOaFVxmQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}